---
title: "Amazon Reviews"
author: "Kevin Kraayeveld (589908)"
date: "2024-03-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(data.table)
library(quanteda)
library(tm)
library(textmineR)
library(dplyr)
library(SnowballC)
library(tidytext)
library(text2vec)
library(ggplot2)
```

```{r}
df <- read.csv("data/fully_cleaned_reviews.csv")

dtm <- CreateDtm(doc_vec = df[, "Review"], # character vector of documents
                 doc_names = df[, "id"])

set.seed(4321)
n_topics=10; # Number of different topics
lda_results <- FitLdaModel(dtm = dtm,
                        k = n_topics, # number of topic
                        burnin = 200 + 10*n_topics,
                        iterations = 700 + 10*n_topics,
                         #alpha: prior on topic probabilities - beta: prior on word probabilities
                        alpha = 0.1,beta = 0.05,
                        optimize_alpha = T,
                        calc_likelihood = T,
                        calc_coherence = T)
```

```{r}
n <- nrow(dtm)
train <- sample(1:n, round(n * 0.80))
dtm_train <- dtm[train,]
dtm_val <- dtm[-train,]

res <- NULL
TMlist <- list()  # list to collect results
n_topics <- 10

for (n_topics in seq(5, 25, 5)) # Do for number of topics 10, 20, 30, 40, 50 
  {
    print(n_topics)
  
    lda_results <- FitLdaModel(dtm = dtm_train,
                         k = n_topics, # number of topic
                         burnin = 200 + 10*n_topics,
                         iterations = 700 + 10*n_topics,
                         alpha = 0.1,beta = 0.05,
                         optimize_alpha = T,
                         calc_likelihood = T,
                         calc_coherence = T) 

    # calculates word co-occurrence for moving window (M=5) and contrasts with topic structure
    coh_train <- mean(CalcProbCoherence(phi = lda_results$phi, dtm = dtm_train, M = 5) )
    coh_val <- mean(CalcProbCoherence(phi = lda_results$phi, dtm = dtm_val, M = 5) )

    # Calculate the log likelihood of the DTM for training set
    ll_train <- CalcLikelihood(dtm = dtm_train, 
                     phi = lda_results$phi, 
                     theta = lda_results$theta)/nrow(dtm_train)
    # Determine theta for validation set
    lda_results$theta_val <- predict(lda_results, dtm_val, method = "gibbs", iterations = 700 + 10*n_topics, burnin = 200 + 10*n_topics)

    # Calculate the log likelihood of the DTM for validation set
    ll_val <- CalcLikelihood(dtm = dtm_val, 
                     phi = lda_results$phi, 
                     theta = lda_results$theta_val)/nrow(dtm_val)

    # Combine all values in matrix per row
    res <- rbind(res, data.frame(n_topics=n_topics, ll_train = ll_train, ll_validation = ll_val, coh_train=coh_train, coh_val=coh_val))

    # Add current LDA result
    TMlist <- append(TMlist, c(lda_results))
  
    print(res)
  }
```

```{r}
# Make plot showing the coherence for different numbers of topics
ggplot(res, aes(n_topics)) +                    
  geom_line(aes(y=coh_train, colour="Train")) + # show coherence training set
  geom_line(aes(y=coh_val, colour="Validation")) # show coherence validation set

# Print results with best coherence
res[which.max(res$coh_val),]

ggplot(res, aes(n_topics)) +                    
  geom_line(aes(y=ll_train, colour="Train")) + # show perplexity training set
  geom_line(aes(y=ll_validation, colour="Validation")) + # show perplexity validation set
  labs(colour="Sample")

# Print results with best perplexity on validation sample (based on likelihood which is easier to read)
res[which.max(res$ll_validation),]
```

```{r}
alpha = .1
n_topics = 15
lda_results <- FitLdaModel(dtm = dtm_train,
                       k = n_topics, # number of topics
                       burnin = 200 + 10*n_topics,
                       iterations = 700 + 10*n_topics,
                       alpha = alpha, beta = 0.05,
                       optimize_alpha = T,
                       calc_likelihood = T,
                       calc_coherence = T)

model <- lda_results
# Get the prevalence of each topic
# You can make this discrete by applying a threshold, say 0.05, for
# topics in/out of docuemnts. 
model$prevalence <- colSums(model$theta) / sum(model$theta) * 100
plot(model$prevalence, model$alpha, xlab = "% topic prevalence", ylab = "alpha")
```

```{r}
maxProb <- apply(lda_results$theta, 1, max)
hist(maxProb, main="Maximum topic probability", xlab="Maximum topic probability")
```

```{r look at term probabilities per topic}
# Get the top M terms of each topic
model$top_terms <- GetTopTerms(phi = model$phi, M = 10)

model$top_terms
```

```{r}
SummarizeTopics(lda_results)
```

```{r}
# Get top 10 words with probabilities for visualization
phi <- lda_results$phi
top_term_table <- NULL
n_topics = 15
for (j in 1:n_topics) {
  words <- t(phi[j,]) %>% order(decreasing=TRUE) %>% head(10) %>% phi[j,.]
  top_term_table <- rbind(top_term_table, data.frame(topic=j,probability=words , term = labels(words)) )
}

```

```{r graphically show most likely terms}
# Create a table of top 10 words grouped by topic
text_top_terms <- top_term_table %>%
  group_by(topic) %>%
  top_n(10, probability) %>%
  ungroup() %>%
  arrange(topic, -probability)

  # Create plots of top 10 words for each topic
  perplot <- 6
  for (i in 1:ceiling(n_topics/perplot))
  {
    p <- text_top_terms %>%
      filter(topic > (i-1)*perplot & topic<=(i*perplot)) %>% # filter out topic
      mutate(term = reorder_within(term, probability, topic)) %>% # show highest probability up top
      ggplot(aes(term, probability, fill = factor(topic))) + # bar plot layout 
      geom_col(show.legend = FALSE) +
      facet_wrap(~ topic, scales = "free") +
      coord_flip()+
      scale_x_reordered()
    show(p)
  }
```