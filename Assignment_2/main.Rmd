---
title: "Assignment 2"
author: "Kevin Kraayeveld (589908)"
date: "2024-04-10"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(data.table)
library(quanteda)
library(tm)
library(dplyr)
library(SnowballC)
library(tidytext)
library(text2vec)
library(parallel)
library(lsa)
library(sentimentr)
```

```{r data_reading, include = FALSE}
df <- fread("AmazonReviews.csv")
df <- df[, isPositive, Review]
```

```{r data_cleaning, include = FALSE}
df$id <- seq_len(nrow(df))

# Lowercase the reviews
df$Review <- char_tolower(df$Review)

# Add additional column for sentiment
df$Review_uncleaned <- df$Review

# Remove multiple punctuations
df[, Review := gsub("\\.+", ".", Review)]
df[, Review := gsub("\\!+", "!", Review)]
df[, Review := gsub("\\?+", "?", Review)]
df[, Review := gsub("\\|+", "|", Review)]

# Convert smileys and times to text 
df$Review <- df$Review %>%
  gsub(":( |-|o)*\\("," sadsmile ", .) %>%
  gsub(":( |-|o)*\\)"," happysmile ", .) %>%
  gsub("([0-9]+:)*[0-9]+ *am"," timeam", .) %>%  # Find time AM
  gsub("([0-9]+:)*[0-9]+ *pm"," timepm", .)  # Find time PM

# Remove punctuation
df$Review <- removePunctuation(df$Review)
# Remove numbers
df$Review <- removeNumbers(df$Review)
# Tokenize reviews
df$Review <- tokens(df$Review)
# Get stop words
data("stop_words")
# Remove a few stop words
ignore_list <- stop_words %>% filter(!word %in% c("no", "not", "never"))
df$Review_bigram <- tokens_select(df$Review, ignore_list$word, selection = "remove")
df$Review <- tokens_select(df$Review, stop_words$word, selection = "remove")
# Stem words
df$Review <- lapply(df$Review, function(token) wordStem(token))
df$Review_bigram <- lapply(df$Review_bigram, function(token) wordStem(token))

df$Review <- tokens_select(as.tokens(df$Review), c("no", "never", "not"), selection = "remove")
df$Review <- as.list(df$Review)

# Create vocabulary
vocabulary <- create_vocabulary(itoken(df$Review))
# Only keep words that are in at least 0.5% of the reviews
pruned_vocabulary <- prune_vocabulary(vocabulary, doc_proportion_min = 0.005)

# Delete words that are not in the pruned vocabulary
words_to_delete <- setdiff(vocabulary$term, pruned_vocabulary$term)
df$Review <- tokens_select(as.tokens(df$Review), words_to_delete, selection = "remove")
df$Review_bigram <- tokens_select(as.tokens(df$Review_bigram), words_to_delete, selection = "remove")
df$Review <- as.list(df$Review)
df$Review_bigram <- as.list(df$Review_bigram)

df$Review_Tokens <- df$Review

# Turn the review back into a text instead of token
df$Review <- lapply(df$Review, function(token) {
  paste(token, collapse = " ")
})

df$Review_bigram <- lapply(df$Review_bigram, function(token) {
  paste(token, collapse = " ")
})

# Remove empty reviews
df <- df[Review != ""]

# Cleaned data set
if (!file.exists("../data/fully_cleaned_reviews.csv")) {
  fwrite(df, "../data/fully_cleaned_reviews.csv")
}

```

```{r glove, echo = FALSE}
set.seed(100)

iter <- itoken(df$Review_Tokens)
vectorizer <- vocab_vectorizer(pruned_vocabulary)
tcm <- create_tcm(it = iter, 
                  vectorizer = vectorizer,
                  skip_grams_window = 5L)

glove_model <- GloVe$new(rank = 50, # Dimensionality of the vector
                         x_max = 100, # maximum number of co-occurrences to use in the weighting function
                         learning_rate = 0.2, # learning rate for SGD
                         alpha = 0.75, # the alpha in weighting function formula
                         lambda = 0, # regularization parameter
                         shuffle = FALSE)

# Set the number of threads
num_cores <- detectCores()
options(mc.cores = num_cores)
                  
print("Train GloVe model")
glove_model$fit_transform(x = tcm, # Co-occurence matrix
                          n_iter = 200, # number of SGD iterations
                          convergence_tol = -1) # defines early stopping strategy

word_embeddings <- glove_model$components
model <- t(as.matrix(word_embeddings))
```

```{r part1}
# Calculate the cosine similarity matrix for the entire set of words
similarity_matrix <- sim2(model)

# To avoid considering self-similarity, replace diagonal with NA
diag(similarity_matrix) <- NA

# Find the highest value in the similarity matrix, ignoring NAs
max_sim <- max(similarity_matrix, na.rm = TRUE)

# Find the position of the highest similarity value
max_pos <- which(similarity_matrix == max_sim, arr.ind = TRUE)

# Extract the words (columns) corresponding to the highest similarity
word1 <- rownames(model)[max_pos[1, 1]]
word2 <- rownames(model)[max_pos[1, 2]]

# Output the result
cat("The most similar pair is:", word1, "and", word2, "with a similarity score of", max_sim, "\n")

```


```{r 1.1}
get_top_similar_words <- function(word, similarity_matrix, top_n = 10) {
  row_index <- which(rownames(similarity_matrix) == word)
  similarities <- similarity_matrix[row_index, ]
  sorted_indices <- order(similarities, decreasing = TRUE) 
  top_indices <- sorted_indices[1:min(length(sorted_indices), top_n)]
  top_words <- rownames(similarity_matrix)[top_indices]
  return(top_words)
}

get_top_similar_words("film", similarity_matrix)
get_top_similar_words("artist", similarity_matrix)
get_top_similar_words("book", similarity_matrix)
```

```{r 1.2}
dimension <- 3
model[which(abs(model[,dimension]) == max(abs(model[,dimension]))),dimension]
{
chosen_dimension <- rep(0,50)
chosen_dimension[dimension] <- model[which(abs(model[,dimension]) == max(abs(model[,dimension]))),dimension]
similarities <- sim2(model, t(chosen_dimension)) 
ranking <- sim2(model, t(chosen_dimension)) %>% order(decreasing = TRUE)
data.frame(similarities[ranking[1:20]], row.names = pruned_vocabulary$term[ranking[1:20]])
}
```

```{r 1.3}
arithmetic <- model[which("movi" == rownames(model)),] - 
  model[which("watch" == rownames(model)),] + 
  model[which("read" == rownames(model)),]

similarities <- sim2(model, t(arithmetic))
ranking <- order(similarities, decreasing = TRUE)
data.frame(similarities[ranking[1:20]], row.names = pruned_vocabulary$term[ranking[1:20]])

```

```{r similar_reviews}
vector_averaging <- function(token, model){
  embeddings <- model[unlist(token),]
  if(is.null(dim(embeddings)[1])){
    vector <- embeddings
  } else{
    vector <- colMeans(embeddings)
  }
  return(vector)
}

df[, Review_Vector := lapply(df$Review_Tokens, function(tokens){
  vector_averaging(tokens, model)
})]

review_matrix <- do.call(rbind, df$Review_Vector)

# Calculate the cosine similarity matrix for the entire set of words
similarity_matrix_reviews <- sim2(review_matrix)

# To avoid considering self-similarity, replace diagonal with NA
diag(similarity_matrix_reviews) <- NA

rownames(similarity_matrix_reviews) <- rownames(df)

get_top_similar_words(100, similarity_matrix_reviews)

df$Review[100]
df$Review[681]
```


```{r 2.1}
# Unigram of words
sel_unigram <- df %>% 
  unnest_tokens(unigram, Review, token = "words") %>%
  count(unigram, sort = TRUE) %>%
  slice(1:50)


unigram_dtm <- df %>% unnest_tokens(unigram, Review, token = "words")  %>%
  count(id, unigram, sort = TRUE) %>% 
  filter(unigram %in% sel_unigram$unigram) %>%
  arrange(id) %>% 
  cast_dtm(id, unigram, n)

unigram_dtm <- as.matrix(unigram_dtm)
```

```{r 2.1}
# Bigrams of words
sel_bigram <- df %>% 
  unnest_tokens(bigram, Review_bigram, token = "ngrams", n = 2 ) %>%
  count(bigram, sort = TRUE) %>%
  filter( n >= 100)


bigram_dtm <- df %>% unnest_tokens(bigram, Review_bigram, token = "ngrams", n = 2 )  %>%
  count(id, bigram, sort = TRUE) %>% 
  filter(bigram %in% sel_bigram$bigram) %>%
  arrange(id) %>% 
  cast_dtm(id, bigram, n)

bigram_dtm <- as.matrix(bigram_dtm)
```


```{r 2.1}
prediction_df <- df[,c("id", "isPositive", "Review_Vector")]

# Sentiment score for each review
prediction_df$sentiment_score <- sentiment_by(sentimentr::get_sentences(df$Review_uncleaned))$ave_sentiment

# Joining bigrams and unigrams into the data frame
prediction_df <- cbind(prediction_df, bigram_dtm[match(prediction_df$id, rownames(bigram_dtm)),])
prediction_df <- cbind(prediction_df, unigram_dtm[match(prediction_df$id, rownames(unigram_dtm)),])
prediction_df[is.na(prediction_df)] <- 0

# Separate the vector embeddings into columns
prediction_df[,paste0("V", 1:50) := transpose(Review_Vector)]
prediction_df[,Review_Vector := NULL]
```


```{r 2.1}
# Spliiting into testing and training
set.seed(100)
train_id <- sample(1:nrow(prediction_df), nrow(prediction_df)*0.8)
df_train <- prediction_df[train_id]
df_test <- prediction_df[-train_id]

x_train <- df_train[,-c("isPositive","id")]
x_test <- df_test[,-c("isPositive","id")]

y_train <- df_train$isPositive
y_test <- df_test$isPositive
```


```{r 2.1}
# Running a logistic regression
logi_1 <- glm(y_train ~., data = x_train, family = "binomial")

df_test$pred_logi <- ifelse(predict(logi_1 ,df_test, type = "response") > 0.5, 1,0)



confusion_matrix <- table(df_test$isPositive, df_test$pred_logi)

TN <- confusion_matrix[[1]]
TP <- confusion_matrix[, 2][[2]]
FP <- confusion_matrix[[2]]
FN <- confusion_matrix[, 2][[1]]

print(confusion_matrix)

# Calculate measures
accuracy <- (TP + TN) / (TP + TN + FP + FN)
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)
specificity <- TN / (TN + FP)
f1_score <- 2 * (precision * recall) / (precision + recall)

# Print results
print(paste("Accuracy:", round(accuracy, 3)))
print(paste("Precision:", round(precision, 3)))
print(paste("Recall:", round(recall, 3)))
print(paste("Specificity:", round(specificity, 3)))
print(paste("F1-score:", round(f1_score, 3)))

```

```{r 2.1}
# Running a lasso regression
library(glmnet)
set.seed(100)

x_train_lasso <- as.matrix(x_train)

lasso_cv <- cv.glmnet(x_train_lasso, y_train, family = "binomial", alpha = 1)
lasso_1se <- lasso_cv$lambda.1se

lasso_1 <- glmnet(x_train_lasso, y_train, alpha = 1, lambda = lasso_1se, family = "binomial")
lasso_1$beta

# Make prediction on test set
df_test$lasso_pred <- ifelse(predict(lasso_1, as.matrix(x_test), type = "response") > 0.5, 1,0)

# See accuracy
confusion_matrix <- table(df_test$isPositive, df_test$lasso_pred)

TN <- confusion_matrix[[1]]
TP <- confusion_matrix[, 2][[2]]
FP <- confusion_matrix[[2]]
FN <- confusion_matrix[, 2][[1]]

print(confusion_matrix)

# Calculate measures
accuracy <- (TP + TN) / (TP + TN + FP + FN)
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)
specificity <- TN / (TN + FP)
f1_score <- 2 * (precision * recall) / (precision + recall)

# Print results
print(paste("Accuracy:", round(accuracy, 3)))
print(paste("Precision:", round(precision, 3)))
print(paste("Recall:", round(recall, 3)))
print(paste("Specificity:", round(specificity, 3)))
print(paste("F1-score:", round(f1_score, 3)))


```
```{r 2.1}
# Random Forest
library(randomForest)

rf_1 <- randomForest(y = y_train,x = x_train, ntree = 100)
df_test$rf_pred <- ifelse(predict(rf_1, df_test, type = "response") > 0.5, 1,0)

# See accuracy
confusion_matrix <- table(df_test$isPositive, df_test$rf_pred)

TN <- confusion_matrix[[1]]
TP <- confusion_matrix[, 2][[2]]
FP <- confusion_matrix[[2]]
FN <- confusion_matrix[, 2][[1]]

print(confusion_matrix)

# Calculate measures
accuracy <- (TP + TN) / (TP + TN + FP + FN)
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)
specificity <- TN / (TN + FP)
f1_score <- 2 * (precision * recall) / (precision + recall)

# Print results
print(paste("Accuracy:", round(accuracy, 3)))
print(paste("Precision:", round(precision, 3)))
print(paste("Recall:", round(recall, 3)))
print(paste("Specificity:", round(specificity, 3)))
print(paste("F1-score:", round(f1_score, 3)))

# What are the most important variables for the RF?
varImpPlot(rf_1)
```







