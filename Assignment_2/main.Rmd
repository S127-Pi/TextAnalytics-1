---
title: "Assignment 2"
author: "Kevin Kraayeveld (589908)"
date: "2024-04-10"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(data.table)
library(quanteda)
library(tm)
library(dplyr)
library(SnowballC)
library(tidytext)
library(text2vec)
library(parallel)
library(lsa)
```

```{r data_reading, include = FALSE}
df <- fread("../data/AmazonReviews.csv")
df <- df[, isPositive, Review]
```

```{r data_cleaning, include = FALSE}
df$id <- seq_len(nrow(df))

# Lowercase the reviews
df$Review <- char_tolower(df$Review)

# Remove multiple punctuations
df[, Review := gsub("\\.+", ".", Review)]
df[, Review := gsub("\\!+", "!", Review)]
df[, Review := gsub("\\?+", "?", Review)]
df[, Review := gsub("\\|+", "|", Review)]

# Convert smileys and times to text 
df$Review <- df$Review %>%
  gsub(":( |-|o)*\\("," sadsmile ", .) %>%
  gsub(":( |-|o)*\\)"," happysmile ", .) %>%
  gsub("([0-9]+:)*[0-9]+ *am"," timeam", .) %>%  # Find time AM
  gsub("([0-9]+:)*[0-9]+ *pm"," timepm", .)  # Find time PM

# Remove punctuation
df$Review <- removePunctuation(df$Review)
# Remove numbers
df$Review <- removeNumbers(df$Review)
# Tokenize reviews
df$Review <- tokens(df$Review)
# Remove stop words
data("stop_words")
df$Review <- tokens_select(df$Review, stop_words$word, selection = "remove")
# Stem words
df$Review <- lapply(df$Review, function(token) wordStem(token))

# Create vocabulary
vocabulary <- create_vocabulary(itoken(df$Review))
# Only keep words that are in at least 0.5% of the reviews
pruned_vocabulary <- prune_vocabulary(vocabulary, doc_proportion_min = 0.005)

# Delete words that are not in the pruned vocabulary
words_to_delete <- setdiff(vocabulary$term, pruned_vocabulary$term)
df$Review <- tokens_select(as.tokens(df$Review), words_to_delete, selection = "remove")
df$Review <- as.list(df$Review)

df$Review_Tokens <- df$Review

# Turn the review back into a text instead of token
df$Review <- lapply(df$Review, function(token) {
  paste(token, collapse = " ")
})

# Remove empty reviews
df <- df[Review != ""]

# Cleaned data set
if (!file.exists("../data/fully_cleaned_reviews.csv")) {
  fwrite(df, "../data/fully_cleaned_reviews.csv")
}

```

```{r glove, echo = FALSE}
set.seed(100)

iter <- itoken(df$Review_Tokens)
vectorizer <- vocab_vectorizer(pruned_vocabulary)
tcm <- create_tcm(it = iter, 
                  vectorizer = vectorizer,
                  skip_grams_window = 5L)

glove_model <- GloVe$new(rank = 50, # Dimensionality of the vector
                         x_max = 100, # maximum number of co-occurrences to use in the weighting function
                         learning_rate = 0.2, # learning rate for SGD
                         alpha = 0.75, # the alpha in weighting function formula
                         lambda = 0, # regularization parameter
                         shuffle = FALSE)

# Set the number of threads
num_cores <- detectCores()
options(mc.cores = num_cores)
                  
print("Train GloVe model")
glove_model$fit_transform(x = tcm, # Co-occurence matrix
                          n_iter = 50, # number of SGD iterations
                          convergence_tol = -1) # defines early stopping strategy

word_embeddings <- glove_model$components
model <- t(as.matrix(word_embeddings))
```

```{r part1}
# Calculate the cosine similarity matrix for the entire set of words
similarity_matrix <- sim2(model)

# To avoid considering self-similarity, replace diagonal with NA
diag(similarity_matrix) <- NA

# Find the highest value in the similarity matrix, ignoring NAs
max_sim <- max(similarity_matrix, na.rm = TRUE)

# Find the position of the highest similarity value
max_pos <- which(similarity_matrix == max_sim, arr.ind = TRUE)

# Extract the words (columns) corresponding to the highest similarity
word1 <- colnames(model)[max_pos[1, 1]]
word2 <- colnames(model)[max_pos[1, 2]]

# Output the result
cat("The most similar pair is:", word1, "and", word2, "with a similarity score of", max_sim, "\n")

```


```{r}
get_top_similar_words <- function(word, top_n = 10) {
  row_index <- which(rownames(similarity_matrix) == word)
  similarities <- similarity_matrix[row_index, ]
  sorted_indices <- order(similarities, decreasing = TRUE) 
  top_indices <- sorted_indices[1:min(length(sorted_indices), top_n)]
  top_words <- rownames(similarity_matrix)[top_indices]
  return(top_words)
}

get_top_similar_words("film")
```

