---
title: "Assignment 2"
author: "Kevin Kraayeveld (589908)"
date: "2024-04-10"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(data.table)
library(quanteda)
library(tm)
library(dplyr)
library(SnowballC)
library(tidytext)
library(text2vec)
library(parallel)
library(lsa)
```

```{r data_reading, include = FALSE}
df <- fread("../data/AmazonReviews.csv")
df <- df[, isPositive, Review]
```

```{r data_cleaning, include = FALSE}
df$id <- seq_len(nrow(df))

# Lowercase the reviews
df$Review <- char_tolower(df$Review)

# Remove multiple punctuations
df[, Review := gsub("\\.+", ".", Review)]
df[, Review := gsub("\\!+", "!", Review)]
df[, Review := gsub("\\?+", "?", Review)]
df[, Review := gsub("\\|+", "|", Review)]

# Convert smileys and times to text 
df$Review <- df$Review %>%
  gsub(":( |-|o)*\\("," sadsmile ", .) %>%
  gsub(":( |-|o)*\\)"," happysmile ", .) %>%
  gsub("([0-9]+:)*[0-9]+ *am"," timeam", .) %>%  # Find time AM
  gsub("([0-9]+:)*[0-9]+ *pm"," timepm", .)  # Find time PM

# Remove punctuation
df$Review <- removePunctuation(df$Review)
# Remove numbers
df$Review <- removeNumbers(df$Review)
# Tokenize reviews
df$Review <- tokens(df$Review)
# Get stop words
data("stop_words")
# Remove a few stop words
ignore_list <- stop_words %>% filter(!word %in% c("no", "not", "never"))
df$Review_bigram <- tokens_select(df$Review, ignore_list$word, selection = "remove")
df$Review <- tokens_select(df$Review, stop_words$word, selection = "remove")
# Stem words
df$Review <- lapply(df$Review, function(token) wordStem(token))
df$Review_bigram <- lapply(df$Review_bigram, function(token) wordStem(token))

df$Review <- tokens_select(as.tokens(df$Review), c("no", "never", "not"), selection = "remove")
df$Review <- as.list(df$Review)

# Create vocabulary
vocabulary <- create_vocabulary(itoken(df$Review))
# Only keep words that are in at least 0.5% of the reviews
pruned_vocabulary <- prune_vocabulary(vocabulary, doc_proportion_min = 0.005)

# Delete words that are not in the pruned vocabulary
words_to_delete <- setdiff(vocabulary$term, pruned_vocabulary$term)
df$Review <- tokens_select(as.tokens(df$Review), words_to_delete, selection = "remove")
df$Review_bigram <- tokens_select(as.tokens(df$Review_bigram), words_to_delete, selection = "remove")
df$Review <- as.list(df$Review)
df$Review_bigram <- as.list(df$Review_bigram)

df$Review_Tokens <- df$Review

# Turn the review back into a text instead of token
df$Review <- lapply(df$Review, function(token) {
  paste(token, collapse = " ")
})

df$Review_bigram <- lapply(df$Review_bigram, function(token) {
  paste(token, collapse = " ")
})

# Remove empty reviews
df <- df[Review != ""]

# Cleaned data set
if (!file.exists("../data/fully_cleaned_reviews.csv")) {
  fwrite(df, "../data/fully_cleaned_reviews.csv")
}

```

```{r glove, echo = FALSE}
set.seed(100)

iter <- itoken(df$Review_Tokens)
vectorizer <- vocab_vectorizer(pruned_vocabulary)
tcm <- create_tcm(it = iter, 
                  vectorizer = vectorizer,
                  skip_grams_window = 5L)

glove_model <- GloVe$new(rank = 50, # Dimensionality of the vector
                         x_max = 100, # maximum number of co-occurrences to use in the weighting function
                         learning_rate = 0.2, # learning rate for SGD
                         alpha = 0.75, # the alpha in weighting function formula
                         lambda = 0, # regularization parameter
                         shuffle = FALSE)

# Set the number of threads
num_cores <- detectCores()
options(mc.cores = num_cores)
                  
print("Train GloVe model")
glove_model$fit_transform(x = tcm, # Co-occurence matrix
                          n_iter = 200, # number of SGD iterations
                          convergence_tol = -1) # defines early stopping strategy

word_embeddings <- glove_model$components
model <- t(as.matrix(word_embeddings))
```

```{r part1}
# Calculate the cosine similarity matrix for the entire set of words
similarity_matrix <- sim2(model)

# To avoid considering self-similarity, replace diagonal with NA
diag(similarity_matrix) <- NA

# Find the highest value in the similarity matrix, ignoring NAs
max_sim <- max(similarity_matrix, na.rm = TRUE)

# Find the position of the highest similarity value
max_pos <- which(similarity_matrix == max_sim, arr.ind = TRUE)

# Extract the words (columns) corresponding to the highest similarity
word1 <- rownames(model)[max_pos[1, 1]]
word2 <- rownames(model)[max_pos[1, 2]]

# Output the result
cat("The most similar pair is:", word1, "and", word2, "with a similarity score of", max_sim, "\n")

```


```{r 1.1}
get_most_similar <- function(word, similarity_matrix, top_n = 10) {
  row_index <- which(rownames(similarity_matrix) == word)
  similarities <- similarity_matrix[row_index, ]
  sorted_indices <- order(similarities, decreasing = TRUE) 
  top_indices <- sorted_indices[1:min(length(sorted_indices), top_n)]
  top_words <- rownames(similarity_matrix)[top_indices]
  return(top_words)
}

get_most_similar("film", similarity_matrix)
get_most_similar("artist", similarity_matrix)
get_most_similar("book", similarity_matrix)
```

```{r 1.2}
dimension <- 3
model[which(abs(model[,dimension]) == max(abs(model[,dimension]))),dimension]
{
chosen_dimension <- rep(0,50)
chosen_dimension[dimension] <- model[which(abs(model[,dimension]) == max(abs(model[,dimension]))),dimension]
similarities <- sim2(model, t(chosen_dimension)) 
ranking <- sim2(model, t(chosen_dimension)) %>% order(decreasing = TRUE)
data.frame(similarities[ranking[1:20]], row.names = pruned_vocabulary$term[ranking[1:20]])
}
```

```{r 1.3}
arithmetic <- model[which("movi" == rownames(model)),] - 
  model[which("watch" == rownames(model)),] + 
  model[which("read" == rownames(model)),]

similarities <- sim2(model, t(arithmetic))
ranking <- order(similarities, decreasing = TRUE)
data.frame(similarities[ranking[1:20]], row.names = pruned_vocabulary$term[ranking[1:20]])

```
```{r similar_reviews}
vector_averaging <- function(token, model){
  embeddings <- model[unlist(token),]
  if(is.null(dim(embeddings)[1])){
    vector <- embeddings
  } else{
    vector <- colMeans(embeddings)
  }
  return(vector)
}

df[, Review_Vector := lapply(df$Review_Tokens, function(tokens){
  vector_averaging(tokens, model)
})]

review_matrix <- do.call(rbind, df$Review_Vector)

# Calculate the cosine similarity matrix for the entire set of words
similarity_matrix_reviews <- sim2(review_matrix)

# To avoid considering self-similarity, replace diagonal with NA
diag(similarity_matrix_reviews) <- NA

rownames(similarity_matrix_reviews) <- rownames(df)

get_most_similar(100, similarity_matrix_reviews)

df$Review[100]
df$Review[681]
```

```{r 2.1}
df <- df %>% unnest_tokens(bigram, Review_bigram, token = "ngrams", n = 2)

```
